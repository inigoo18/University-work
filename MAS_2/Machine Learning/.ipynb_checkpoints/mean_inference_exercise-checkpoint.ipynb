{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import math\n",
    "from pyro.optim import SGD, Adam\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we the simple generative model from Slide 18, which you also experimented with in the notebook *student_BBVI.ipynb*:\n",
    " * https://www.moodle.aau.dk/mod/resource/view.php?id=1049031\n",
    "\n",
    "In the previous notebook we derived the required gradients manually. Here we instead rely on differentiation functionality in Pyro, which, in turn is based n PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model in plate notation\n",
    "\n",
    "<img src=\"https://www.moodle.aau.dk/pluginfile.php/1695750/mod_folder/content/0/mean_model.png?forcedownload=1\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model defined in Pyro\n",
    "\n",
    "Here we define the probabilistic model. Notice the close resemblance with the plate specification above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_model(data):\n",
    "\n",
    "    # Define the random variable mu having a normal distribution as prior\n",
    "    mu = pyro.sample(\"mu\", dist.Normal(0.0,1000.0))\n",
    "\n",
    "    # and now the plate holding the observations. The number of observations are determined by the data set \n",
    "    # supplied to the function. \n",
    "    with pyro.plate(\"x_plate\"):\n",
    "        pyro.sample(f\"X\", dist.Normal(mu, 1), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The variational distribution\n",
    "\n",
    "In Pyro the variational distribution is defined as a so-called guide. In this example our variational distribution is a beta distribution with parameters q_alpha and q_beta:\n",
    "\n",
    "$$\n",
    "q(\\mu)= \\mathit{Normal}(\\mu | q_{mu}, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_guide(data):\n",
    "\n",
    "    # We initialize the variational parameter to 0.0. \n",
    "    q_mu = pyro.param(\"q_mu\", torch.tensor(0.0))\n",
    "\n",
    "    # The name of the random variable of the variational distribution must match the name of the corresponding\n",
    "    # variable in the model exactly.\n",
    "    pyro.sample(\"mu\", dist.Normal(q_mu, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Here we encapsulate the learning steps, relying on standard stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data):\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    elbo = pyro.infer.Trace_ELBO()\n",
    "    svi = pyro.infer.SVI(model=mean_model,\n",
    "                         guide=mean_guide,\n",
    "                         optim=SGD({'lr':0.0001}),\n",
    "                         loss=elbo)\n",
    "\n",
    "    num_steps = 1000\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Loss for iteration {step}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iteration 0: 5436.3763236403465\n",
      "Loss for iteration 50: 1937.3406717777252\n",
      "Loss for iteration 100: 698.8343402147293\n",
      "Loss for iteration 150: 414.3496378660202\n",
      "Loss for iteration 200: 319.3915444612503\n",
      "Loss for iteration 250: 149.88095009326935\n",
      "Loss for iteration 300: 257.7423424720764\n",
      "Loss for iteration 350: 189.83737969398499\n",
      "Loss for iteration 400: 145.53304851055145\n",
      "Loss for iteration 450: 144.2721302509308\n",
      "Loss for iteration 500: 153.16068708896637\n",
      "Loss for iteration 550: 145.11027246713638\n",
      "Loss for iteration 600: 190.50959634780884\n",
      "Loss for iteration 650: 148.2471822500229\n",
      "Loss for iteration 700: 147.65406131744385\n",
      "Loss for iteration 750: 211.7738790512085\n",
      "Loss for iteration 800: 288.0651717185974\n",
      "Loss for iteration 850: 150.1859109401703\n",
      "Loss for iteration 900: 183.0964126586914\n",
      "Loss for iteration 950: 144.34351563453674\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(np.random.normal(loc=10.0, scale=1.0, size=100),dtype=torch.float)\n",
    "learn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the learned variational parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The learned parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmu = pyro.param(\"q_mu\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of vaiational distribution: 10.066452026367188\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean of vaiational distribution: {qmu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "* Adapt the code above to accomodate a slight more rich variational distribution, where we also have a variational parameter for the standard deviation:\n",
    "$$\n",
    "q(\\mu)= \\mathit{Normal}(\\mu | q_{mu}, q_{std})\n",
    "$$\n",
    "* Experiment with different data sets and parameter values. Try visualizing the variational posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_model(data):\n",
    "\n",
    "    # Define the random variable mu having a normal distribution as prior\n",
    "    mu = pyro.sample(\"mu\", dist.Normal(0.0,1000.0))\n",
    "    log_std = pyro.sample(\"std\", dist.Normal(0,1))\n",
    "    std = math.exp(log_std)\n",
    "\n",
    "    # and now the plate holding the observations. The number of observations are determined by the data set \n",
    "    # supplied to the function. \n",
    "    with pyro.plate(\"x_plate\"):\n",
    "        pyro.sample(f\"X\", dist.Normal(mu, std), obs=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_guide(data):\n",
    "\n",
    "    # We initialize the variational parameter to 0.0. \n",
    "    q_mu = pyro.param(\"q_mu\", torch.tensor(0.0))\n",
    "    q_std = pyro.param(\"q_std\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "\n",
    "    # The name of the random variable of the variational distribution must match the name of the corresponding\n",
    "    # variable in the model exactly.\n",
    "    pyro.sample(\"mu\", dist.Normal(q_mu, 1,0))\n",
    "    pyro.sample(\"std\", dist.Normal(q_std, 1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_new(data):\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    elbo = pyro.infer.Trace_ELBO()\n",
    "    svi = pyro.infer.SVI(model=mean_std_model,\n",
    "                         guide=mean_std_guide,\n",
    "                         optim=SGD({'lr':0.0001}),\n",
    "                         loss=elbo)\n",
    "\n",
    "    num_steps = 1000\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Loss for iteration {step}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iteration 0: 687.7784962654114\n",
      "Loss for iteration 50: 387.34066212177277\n",
      "Loss for iteration 100: 381.74849677085876\n",
      "Loss for iteration 150: 310.92422914505005\n",
      "Loss for iteration 200: 257.03968846797943\n",
      "Loss for iteration 250: 223.47096055746078\n",
      "Loss for iteration 300: 193.379743039608\n",
      "Loss for iteration 350: 272.50359547138214\n",
      "Loss for iteration 400: 142.11035233736038\n",
      "Loss for iteration 450: 163.9282724261284\n",
      "Loss for iteration 500: 222.60163289308548\n",
      "Loss for iteration 550: 142.84347289800644\n",
      "Loss for iteration 600: 260.3253366947174\n",
      "Loss for iteration 650: 163.1066094636917\n",
      "Loss for iteration 700: 196.40394812822342\n",
      "Loss for iteration 750: 146.22518855333328\n",
      "Loss for iteration 800: 165.56290924549103\n",
      "Loss for iteration 850: 221.4096195101738\n",
      "Loss for iteration 900: 276.9619903564453\n",
      "Loss for iteration 950: 180.16667711734772\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(np.random.normal(loc=10.0, scale=1.0, size=100),dtype=torch.float)\n",
    "learn_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of vaiational distribution: 9.750102043151855\n"
     ]
    }
   ],
   "source": [
    "qmu = pyro.param(\"q_mu\").item()\n",
    "print(f\"Mean of vaiational distribution: {qmu}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
